# --
# config file for this project


# --
# dataset config
datasets:

  # speech commands dataset
  speech_commands:

    # file extension for audio files
    file_ext: '.wav'

    # path to speech command dataset
    dataset_path: ignore/dataset/speech_commands_v0.01/

    # paths to save training, testing and eval data
    data_paths:
      train: ignore/dataset/train/
      test: ignore/dataset/test/
      eval: ignore/dataset/eval/

    # folder names
    wav_folder: wavs/
    annotation_folder: annotation/

    # split of dataset in training, test, eval (must be a prob. distribution)
    split_percs: [0.8, 0.1, 0.1]

    # plot path root
    plot_paths:
      mfcc: ignore/dataset/plots/mfcc/
      z_score: ignore/dataset/plots/z_score/
      examples_grid: ignore/dataset/plots/

    # enable plot of each feature extraction
    enable_plot: False

    # recreate all datasets (copy wavs to data_paths - not necessary except the dataset is changed)
    recreate: False

    # selected labels of the whole set
    sel_labels: ['left', 'right', 'up', 'down', 'go']
    #sel_labels: ['eight', 'sheila', 'nine', 'yes', 'one', 'no', 'left', 'tree', 'bed', 'bird', 'go', 'wow', 'seven', 'marvin', 'dog', 'three', 'two', 'house', 'down', 'six', 'five', 'off', 'right', 'cat', 'zero', 'four', 'stop', 'up', 'on', 'happy']

    # --
    # version
    # 1: better onset detection
    # 2: energy frame onset detection
    # 3: data extraction with min energy and randomize onsets
    # 4: normalized features
    version_nr: 4

    # number of examples picked from dataset
    #n_examples: 10
    n_examples: 500

    # file name for saved feature files
    feature_file_name: mfcc_data


  # my recordings dataset
  my_recordings:

    # file extension for audio files
    file_ext: '.wav'

    # path to speech command dataset
    dataset_path: ignore/my_recordings/raw/

    # paths to save training, testing and eval data
    data_paths:
      my: ignore/my_recordings/my/

    # folder names
    wav_folder: wavs/
    annotation_folder: annotation/

    # split of dataset
    split_percs: [1.0]

    # plot path root
    plot_paths:
      mfcc: ignore/my_recordings/plots/mfcc/
      z_score: ignore/my_recordings/plots/z_score/
      onsets: ignore/my_recordings/plots/onsets/
      waveform: ignore/my_recordings/plots/waveform/
      examples_grid: ignore/my_recordings/plots/

    # enable plot of each feature extraction
    enable_plot: True

    # recreate all datasets (copy wavs to data_paths - not necessary except the dataset is changed)
    recreate: False

    # selected labels of the whole set
    sel_labels: ['left', 'right', 'up', 'down', 'go']
    #sel_labels: ['eight', 'sheila', 'nine', 'yes', 'one', 'no', 'left', 'tree', 'bed', 'bird', 'go', 'wow', 'seven', 'marvin', 'dog', 'three', 'two', 'house', 'down', 'six', 'five', 'off', 'right', 'cat', 'zero', 'four', 'stop', 'up', 'on', 'happy']

    # --
    # version
    # 1: better onset detection
    # 2: energy frame onset detection
    # 3: data extraction with min energy and randomize onsets
    # 4: normalized features
    version_nr: 4

    # number of examples picked from dataset
    n_examples: 5

    # file name for saved feature files
    feature_file_name: mfcc_data_my


# --
# feature params for dataset extraction
feature_params:

  # sampling rate
  fs: 16000

  # window and hop size [sec]
  N_s: 0.025
  hop_s: 0.010

  # number of filter bands and cepstral coeffs
  n_filter_bands: 32
  n_ceps_coeff: 12

  # frame size of output features -> input to nn
  #frame_size: 32
  frame_size: 50

  # normalized features -> [0, 1] over frames
  norm_features: True

  # with deltas
  compute_deltas: False
  #compute_deltas: True


# --
# machine learning settings
ml:

  # paths
  paths:
    log: ignore/logs/
    model: ignore/models/ 
    model_pre: ignore/models/pre/

  # folders in model path
  model_path_folders:
    diff_plots: diff_plots/

  # file names
  model_file_name: model.pth
  params_file_name: params.npz
  metrics_file_name: metrics.npz
  info_file_name: info.txt

  # encoder model file name
  encoder_model_file_name: encoder_model.pth
  decoder_model_file_name: decoder_model.pth
  #encoder_model_init_file_name: encoder_model_init.pth
  #decoder_model_init_file_name: decoder_model_init.pth

  # use a pre trained model at pre trained model path
  load_pre_model: False

  # saves the model as pre trained model
  save_as_pre_model: False

  # params
  train_params:

    # batch size
    batch_size: 32
    #batch_size: 128

    # number of epochs
    num_epochs: 500

    # learning rate
    lr: 0.0001

    # momentum
    beta: 0.5

  # adversarial parameters
  adv_params:

    # conv coder folders
    conv_folder: 'conv_coder/'

    # train each label with adv
    label_train: True

    # adv pre-training of full network
    pre_train: True

    # iterating algorithm of label: num_iterations_label x num_epochs_label
    num_iterations_label: 2

    # label epochs
    num_epochs_label: 500

    # archs for label train:
    #nn_archs_label: ['conv-experimental', 'adv-experimental']
    #nn_archs_label: ['conv-experimental']
    #nn_archs_label: ['adv-experimental']
    #nn_archs_label: ['conv-experimental', 'adv-experimental3']
    #nn_archs_label: ['adv-experimental3', 'conv-experimental']
    nn_archs_label: ['adv-experimental3']

    # pre epochs
    num_epochs_pre: 500

    # use decoder weights for further training
    use_decoder_weights: True

  # retrain existing model
  #retrain: True
  retrain: False

  # nn architecture available
  nn_architectures: {1: 'conv-trad', 2: 'conv-fstride', 3: 'adv-experimental', 4: 'conv-experimental', 5: 'conv-encoder'}

  # choose architecture
  #nn_arch: conv-experimental
  #nn_arch: adv-experimental
  #nn_arch: conv-fstride
  #nn_arch: conv-trad
  nn_arch: conv-encoder
  #nn_arch: conv-lim-encoder
  #nn_arch: conv-latent

  # logging enable
  logging_enabled: True

  # use cpu
  use_cpu: True

  # plot animation
  plot_animation: False


# --
# test bench
test_bench:

  # plot path
  paths:
    main_path: ignore/test_bench/
    models_path: ignore/test_bench/test_models/

  # plot path root
  plot_paths:
    shift: ignore/test_bench/plots/shift/
    shift_wavs: ignore/test_bench/plots/shift/wavs/
    noise: ignore/test_bench/plots/noise/
    noise_wavs: ignore/test_bench/plots/noise/wavs/

  # test wavs
  test_wavs:
    - ignore/my_recordings/clean_records/left.wav
    - ignore/my_recordings/clean_records/right.wav
    - ignore/my_recordings/clean_records/up.wav
    - ignore/my_recordings/clean_records/down.wav
    - ignore/my_recordings/clean_records/go.wav

  # path to model
  #test_model_path: ignore/test_bench/models/test_model1/
  #test_model_path: ignore/test_bench/models/test_model2/
  #test_model_path: ignore/test_bench/test_models/conv-fstride/
  test_model_path: ignore/test_bench/test_models/conv-encoder/
  #test_model_path: ignore/test_bench/test_models/conv-trad/


  # file names
  #model_file_name: 'model.pth'
  model_file_name: 'cnn_model.pth'
  params_file_name: 'params.npz'


# --
# classifier
classifier:

  # path to model
  model_path: 'models/conv-fstride/v3_c-5_n-2000/bs-32_it-1000_lr-1e-05/'
  #model_path: 'models/conv-encoder/v4_c-5_n-500_f-13x50/bs-32_it-1024_lr-0p0001/'

  # file names
  model_file_name: 'model.pth'
  params_file_name: 'params.npz'

  # verbose
  verbose: False


# --
# microphone parameters
mic_params:

  # slect specific device
  select_device: True

  # sel device id
  device: 0

  # sampling frequency of your microphone device (idealy this would be 16000, as it will be downsampled to it)
  fs_device: 48000

  # channels to be recorded
  channels: 1

  # energy threshold for onsets
  energy_thres: 0.0001

  # collector update size (pre frames)
  update_size: 32

  # collector post frames (after onset detection)
  frames_post: 32

  # plot of mic
  plot_path: 'ignore/mic/'

  # enable plot (set false when playing, just for debugging)
  enable_plot: False


# --
# game settings
game:

  # frames per seconds
  fps: 60

  # size of display
  screen_size: [640, 480]

  # enabled flag
  capture_enabled: False

  # capture path
  paths:
    capture_path: ignore/capture/
    frame_path: ignore/capture/frames/




