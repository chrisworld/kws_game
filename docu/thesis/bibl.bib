% --
% my bib


% --
% history

@ARTICLE{Rosenblatt1958,
    author = {F. Rosenblatt},
    title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
    journal = {Psychological Review},
    year = {1958},
    pages = {65--386}
}

@book{Rosenblatt1962,
      author        = "Rosenblatt, Frank",
      title         = "{Principles of neurodynamics: perceptions and the theory
                       of brain mechanisms}",
      publisher     = "Spartan",
      address       = "Washington, DC",
      year          = "1962",
      url           = "https://cds.cern.ch/record/239697",
}

@inproceedings{Rumelhart1986,
  title={Learning internal representations by error propagation},
  author={D. Rumelhart and Geoffrey E. Hinton and R. J. Williams},
  year={1986}
}

@InProceedings{LeCun1986,
author="Le Cun, Yann",
editor="Bienenstock, E.
and Souli{\'e}, F. Fogelman
and Weisbuch, G.",
title="Learning Process in an Asymmetric Threshold Network",
booktitle="Disordered Systems and Biological Organization",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="233--240",
abstract="Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the ``ideal'' network state for each task is available from the environment. It is possible to use a set of so-called ``hidden units'' [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.",
isbn="978-3-642-82657-3"
}




% papers conv nets

@ARTICLE{LeCun1998,  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}


% papers speech

@article{warden2018,
  title={Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author={Pete Warden},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03209}
}

@inproceedings{sainath2015,
  title={Convolutional neural networks for small-footprint keyword spotting},
  author={Tara N. Sainath and Carolina Parada},
  booktitle={INTERSPEECH},
  year={2015}
}