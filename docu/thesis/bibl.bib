% --
% my bib


% --
% history

% perceptron I
@ARTICLE{Rosenblatt1958,
    author = {F. Rosenblatt},
    title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
    journal = {Psychological Review},
    year = {1958},
    pages = {65--386}
}

% perceptron II
@book{Rosenblatt1962,
      author        = "Rosenblatt, Frank",
      title         = "{Principles of neurodynamics: perceptions and the theory
                       of brain mechanisms}",
      publisher     = "Spartan",
      address       = "Washington, DC",
      year          = "1962",
      url           = "https://cds.cern.ch/record/239697",
}

% backprop
@inproceedings{Rumelhart1986,
  title={Learning internal representations by error propagation},
  author={D. Rumelhart and Geoffrey E. Hinton and R. J. Williams},
  year={1986}
}

% backprop
@InProceedings{LeCun1986,
author="Le Cun, Yann",
editor="Bienenstock, E.
and Souli{\'e}, F. Fogelman
and Weisbuch, G.",
title="Learning Process in an Asymmetric Threshold Network",
booktitle="Disordered Systems and Biological Organization",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="233--240",
abstract="Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the ``ideal'' network state for each task is available from the environment. It is possible to use a set of so-called ``hidden units'' [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.",
isbn="978-3-642-82657-3"
}

% good intro to neural nets
@ARTICLE{LeCun1998,  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}

% mfcc basic
@ARTICLE{Mermelstein1980,
  author={Davis, S. and Mermelstein, P.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}, 
  year={1980},
  volume={28},
  number={4},
  pages={357-366},
  doi={10.1109/TASSP.1980.1163420}
}

% statistical learning
@book{Vapnik1995,
author = {Vapnik, Vladimir N.},
title = {The Nature of Statistical Learning Theory},
year = {1995},
isbn = {0387945598},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}




% --
% dataset

% speech commands set
@article{Warden2018,
  title={Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author={Pete Warden},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03209}
}


% --
% benchmark networks

% transformer network -> highes accuracy
@misc{Berg2021,
      title={Keyword Transformer: A Self-Attention Model for Keyword Spotting}, 
      author={Axel Berg and Mark O'Connor and Miguel Tairum Cruz},
      year={2021},
      eprint={2104.00769},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}


% --
% small-footprint or efficient

% sainath small footprint
@inproceedings{Sainath2015,
  title={Convolutional neural networks for small-footprint keyword spotting},
  author={Tara N. Sainath and Carolina Parada},
  booktitle={INTERSPEECH},
  year={2015}
}

% tu graz, resource efficient: https://arxiv.org/abs/2012.10138
@misc{Peter2020,
  title={Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization}, 
  author={David Peter and Wolfgang Roth and Franz Pernkopf},
  year={2020},
  eprint={2012.10138},
  archivePrefix={arXiv},
  primaryClass={eess.AS}
}


% --
% adversarial

% adv paper I
@misc{Goodfellow2014,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


% --
% transfer learning

% info website:
% https://cs231n.github.io/transfer-learning/


% --
% resnets
%https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/