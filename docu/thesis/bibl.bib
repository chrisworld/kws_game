% --
% my bib


% --
% history

% perceptron I
@ARTICLE{Rosenblatt1958,
    author = {F. Rosenblatt},
    title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
    journal = {Psychological Review},
    year = {1958},
    pages = {65--386}
}

% perceptron II
@book{Rosenblatt1962,
      author        = "Rosenblatt, Frank",
      title         = "{Principles of neurodynamics: perceptions and the theory
                       of brain mechanisms}",
      publisher     = "Spartan",
      address       = "Washington, DC",
      year          = "1962",
      url           = "https://cds.cern.ch/record/239697",
}

% backprop
@inproceedings{Rumelhart1986,
  title={Learning internal representations by error propagation},
  author={D. Rumelhart and Geoffrey E. Hinton and R. J. Williams},
  year={1986}
}

% backprop
@InProceedings{LeCun1986,
author="Le Cun, Yann",
editor="Bienenstock, E.
and Souli{\'e}, F. Fogelman
and Weisbuch, G.",
title="Learning Process in an Asymmetric Threshold Network",
booktitle="Disordered Systems and Biological Organization",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="233--240",
abstract="Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the ``ideal'' network state for each task is available from the environment. It is possible to use a set of so-called ``hidden units'' [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.",
isbn="978-3-642-82657-3"
}

% good intro to neural nets
@ARTICLE{LeCun1998,  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}

% mfcc basic
@ARTICLE{Mermelstein1980,
  author={Davis, S. and Mermelstein, P.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}, 
  year={1980},
  volume={28},
  number={4},
  pages={357-366},
  doi={10.1109/TASSP.1980.1163420}
}

% statistical learning
@book{Vapnik1995,
author = {Vapnik, Vladimir N.},
title = {The Nature of Statistical Learning Theory},
year = {1995},
isbn = {0387945598},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

% svm
@article{Cortes1995,
  author = {Cortes, Corinna and Vapnik, Vladimir},
  title = {Support-Vector Networks},
  year = {1995},
  issue_date = {Sept. 1995},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {20},
  number = {3},
  issn = {0885-6125},
  url = {https://doi.org/10.1023/A:1022627411411},
  doi = {10.1023/A:1022627411411},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  journal = {Mach. Learn.},
  month = sep,
  pages = {273–297},
  numpages = {25},
  keywords = {efficient learning algorithms, radial basis function classifiers, polynomial classifiers, pattern recognition, neural networks}
}

% deep
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}
      






% --
% dataset

% speech commands set
@article{Warden2018,
  title={Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author={Pete Warden},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03209}
}


% --
% benchmark networks

% transformer network -> highes accuracy
@misc{Berg2021,
      title={Keyword Transformer: A Self-Attention Model for Keyword Spotting}, 
      author={Axel Berg and Mark O'Connor and Miguel Tairum Cruz},
      year={2021},
      eprint={2104.00769},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}


% --
% small-footprint or efficient

% sainath small footprint
@inproceedings{Sainath2015,
  title={Convolutional neural networks for small-footprint keyword spotting},
  author={Tara N. Sainath and Carolina Parada},
  booktitle={INTERSPEECH},
  year={2015}
}

% tu graz, resource efficient: https://arxiv.org/abs/2012.10138
@misc{Peter2020,
  title={Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization}, 
  author={David Peter and Wolfgang Roth and Franz Pernkopf},
  year={2020},
  eprint={2012.10138},
  archivePrefix={arXiv},
  primaryClass={eess.AS}
}


% --
% adversarial

% adv paper I
@misc{Goodfellow2014,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


% --
% transfer learning

% info website:
% https://cs231n.github.io/transfer-learning/


% --
% resnets
%https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/