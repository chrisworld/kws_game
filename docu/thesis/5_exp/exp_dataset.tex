% --
% dataset

\section{Dataset}\label{sec:exp_dataset}
Two datasets are used within this thesis, one is the second version of the speech commands dataset from \cite{Warden2018} and one is self made, denoted as \enquote{my dataset} which consists of only 5 labels that are especially valuable for movement in video games.

%(\enquote{left}, \enquote{right}, \enquote{right}, \enquote{right}).

Note that the \enquote{my dataset} is merely used for evaluation.
The training, validation and also testing of the neural network architectures is done on the speech commands dataset.
Both datasets consists of raw waveform files in the \texttt{.wav} format, no feature extraction was done beforehand.
As already mentioned in \rsec{prev_kws_benchmark} direct comparisons between different neural network approaches is a bit difficult if the feature extraction is left to the user alone.
Some datasets provide feature extraction beforehand, so that the comparability of neural network architectures performances is not influenced on it.
The speech commands dataset does no explicit separation into train, test and validation sets, but provides file lists that refers to distinct waveform files that should be used for test and validation.
More details of the datasets are presented below.


% Some abbreviations and references were done, so that the jungle of selected parameters get a little bit more clear to the reader of this thesis.
% The abbreviations of the dataset are shown in \rtab{exp_dataset_abbr}.

% The speech commands dataset is extracted before it is used for training. 
% To reduce computations in the evaluation process of neural networks, it was important to reduce the number of classes and examples per class to an suitable number.

% \input{./5_exp/tables/tab_exp_dataset_abbr.tex}

% --
% speech commands dataset

\subsection{Speech Commands Dataset}\label{sec:exp_dataset_speech_cmd}
The speech command dataset \cite{Warden2018} consists of \SI{1}{\second} recordings from more than 30 different words, done by over thousands of different speakers.
The examples within the dataset were not recorded by professionals with high-end recording equipment, in fact their recording was done in an amateur kind of fashion, so that the dataset is more suited to realistic environments intended for user applications.
This is also noted in the paper \cite{Warden2018}:
\begin{quote}
...This meant that the use of studio-captured samples seemed unrealistic, since that audio would lack background noise, would be captured with high-quality microphones, and in a formal setting. 
Successful models would need to cope with noisy environments, poor quality recording equipment, and people talking in a natural, chatty way...
\end{quote}
The recording devices of the speakers, who contributed examples to the dataset, were in most cases simple consumer microphones, as for instance deployed in laptops or mobile phones.

The personal experiments made, when listening to the examples in the dataset, were:
\begin{itemize}
  \item The quality of the examples in the dataset are ranging from really good and understandable to very bad, noisy and unrecognizable, though most of the examples are good.

  \item Different accents can be perceived, that suggests that people from several countries were involved. However the bias is more on American English as noted in the paper.

  \item No children speakers were found on the personal listening.
\end{itemize}

Due to data privacy issues the information on the individual speakers are not given.
Further it is not clear if there are equal amounts of male and female speakers and if there are any children speakers included.
The last would be especially interesting for a video games suited for kids.

In many recordings the background noise is imminent, such as traffic noise, chattering people, office sounds, etc.
Some quality check of the recorded files in the dataset was done by the author of \cite{Warden2018} to reject bad samples.
However there are still some existing flaws such as too loud or too silent files or examples with inconsistent sample numbers or some examples that are prone with too much noise or in the worst case, noise only.
Those quality issues in the dataset are for most cases neglectable or can be fixed, such as inconsistent sample numbers. 
Other more problematic cases, for instance noise-only examples, should be filtered out.
%Still it is a great dataset, because there is no need for a perfect dataset when working with neural networks and one can be happy that there exists one with this amount of diversity and free of access under the creative common license.
Usually its not a problem for neural networks to cope with unclean datasets, because of their ability to learn invariance against noise, loudness differences and other nuisances during training.
Further if the training dataset is large enough and the test and validation sets do not include really bad examples, then there should be no big issues.
However some cleanu up of the dataset was still done as described in...

The speech command dataset exists in two versions (\texttt{v0.01} and \texttt{v0.02}), the first one was published in 2017 and the second one emerged as improved version of \texttt{v0.01} with more key words, examples and quality in 2018.
In this thesis, the experiments are done on the second version \texttt{v0.02}.

Some examples of the speech command dataset in raw audio format are shown in \rfig{exp_dataset_wav_grid_c30} and give a small glimpse on the quality of the recordings.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{./5_exp/figs/exp_dataset_wav_grid_c30}
  \caption{One random sample of each individual speech command in the speech command dataset in normalized raw audio format.}
  \label{fig:exp_dataset_wav_grid_c30}
\end{figure}
\FloatBarrier
\noindent

% dataset structure
\subsubsection{Dataset Structure}
The speech command examples are stored in separate folders, named after each individual speech command, in the \texttt{.wav} format.
The folder named as \texttt{\_background\_noise\_} contains six different background noise files, such as \texttt{white\_noise.wav} or \texttt{doing\_the\_dishes.wav}, with a duration of more than one minute each.
Noise examples with a new noise label named \texttt{_noise} were extracted from those background noise files with a \SI{1}{\second} window shifted by \SI{0.2}{\second}.

Each waveform file is named with an 8-digit hexadecimal hash code for the speaker identification, followed by the utterance number, for instance \texttt{3b4f8f24\_nohash\_0.wav}.
Therefore it is possible to distinguish between different speakers, however as mentioned above, no further information about the speaker is given due to data privacy issues.

%It has to be mentioned, that it is wonderful, that a dataset for simple key word spotting on speech commands with this amount of diversity and free of access under the creative common license exist.


% --
% my dataset

\subsection{My own Dataset}\label{sec:exp_dataset_my}
This dataset was created by the author of this thesis and contains five examples samples each from the words \{\enquote{left}, \enquote{right}, \enquote{up}, \enquote{down} and \enquote{go}\}.
The datasets purpose is mainly to have an additional test set for evaluating trained models on the authors own voice with different word pronouncement on each example.
It is important to mention that none of the self recorded files were used within the training set, so that the neural networks performance on this unseen data is evaluated.
All examples of my own dataset are illustrated in \rfig{exp_dataset_wav_grid_my} in raw audio format.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{./5_exp/figs/exp_dataset_wav_grid_my}
  \caption{Self recorded files of the \enquote{my dataset} in raw audio format.}
  \label{fig:exp_dataset_wav_grid_my}
\end{figure}
\FloatBarrier
\noindent
The examples per word are spoken with different emphasis and stress on individual phonemes.
Also the prolongation of the words are different, that in one example the word is fast and in the other its slow spoken.
The emphasis and prolongation ensure the diversity of the dataset. 
It turned out that it is not easy for neural networks to gain a $100\%$ classification score upon it, even though there is only one and the same speaker.

