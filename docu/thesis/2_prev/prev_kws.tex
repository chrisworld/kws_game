% --
% prev key word spotting

\section{Key Word Spotting with Neural Networks}\label{sec:prev_kws}
\thesisStateNotReady
Some important works regarding KWS with neural networks are presented here. 
The aspect of energy efficients is very important within this thesis, because of the deployment of a KWS system in a video game.
Further the neural network architectures evaluated in this thesis are trained and tested on an already profoundly examined speech command dataset \cite{Warden2018} with many different solution concepts operating with neural networks. 
A benchmark is therefore given on classification accuracies on the test set of this dataset.


% --
% energy efficient

\subsection{Energy efficient solutions}
In video games the processing and classification of speech commands has to run in real-time and therefore a low computational footprint is needed for the neural networks.
One of the most famous papers on low computational footprint regarding KWS systems is from Sainath et. al. in 2015 \cite{Sainath2015}.
Two neural networks architectures are chosen from this paper, one is a traditional CNN network for comparison and the other is a limited multipliers network with a CNN striding only in the frequency axis.
Both networks are described in detail in \rsec{nn_arch}.

A deployment of a KWS system on microcontrollers is examined in \cite{Zhang2017}. 
Different neural network architectures were evaluated regarding their memory usage and operations per inference.


% --
% benchmark

\subsection{Benchmark Networks for this thesis}\label{sec:prev_kws_benchmark}
First it is to mention that the speech command dataset \cite{Warden2018} consists of raw audio data in the \texttt{.wav} format and there is no pre-processing or feature extraction done beforehand.
Also it is up to the users for which labels are to be selected.
The main idea however is to choose the \emph{core words} as classification labels and add an unkown label for the \emph{auxiliary words} words.
Also a separate noise or silence label can be added from given noise data files.
More details are presented in \rsec{exp_dataset}.

Therefore it is difficult to compare scores between two different papers.
Still the classification task is not an easy one in any constellation of chosen labels or feature extraction and usually everything that has a accuracy score over 85\% for at least 7 - 10 labels is already pretty good.
A good overview of actual benchmark scores regarding the speech command dataset is given in \cite{PaperswithcodeKWS}.



