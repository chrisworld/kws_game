\section{Historical Remarks on Neural Networks}
The first step towards computational Neural Networks, as we know them today, was the introduction of the so called \enquote{Perceptron} by Rosenblatt in the year 1958 \cite{Rosenblatt1958}. 
The idea of the Perceptron emerged from physiologists, trying to model a physiological Neural Network in computational terms. 
This first model was based on the information processing of the retina (input nodes), which passes through several physiological Neural Networks (hidden nodes) and finally elicit an action or decision (output nodes).
With his work and implementation in a computer system, Rosenblatt kicked of the domain of computational learning systems and many different computational Neural Network architectures appeared through the time.

Another big advance in Neural Network history, was the introduction of a very famous learning algorithm known as \enquote{Backpropagation}, evolved by several authors at the same time \cite{LeCun1986} and \cite{Rumelhart1986} in the late 80s. Even nowadays, 35 years after introducing Backpropagation, it is still the \enquote{de facto} standard in training Neural Networks and implemented in every Machine Learning framework e.g. \texttt{Pytorch} or \texttt{Tensorflow} as core element.